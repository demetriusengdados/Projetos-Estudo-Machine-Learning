{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Purple'>Ciência dos Dados na Pratica</font>\n",
    "# <font color='blue'>Analise de Sentimentos em Tempo Real no Twitter</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *********** Atenção: *********** \n",
    "Utilize Java JDK 1.8 / Apache Spark 2.4.2  / Python 3.7 / Ubuntu 18.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** Caso receba mensagem de erro \"name 'sc' is not defined\", interrompa o pyspark e apague o diretório metastore_db no mesmo diretório onde está este Jupyter notebook ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse http://localhost:4040 sempre que quiser acompanhar a execução dos jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos usar o Framework Spark Streaming para processar dados do Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_oauthlib\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests_oauthlib) (2.22.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests_oauthlib) (2019.11.28)\n",
      "Installing collected packages: oauthlib, requests-oauthlib\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0\n",
      "Collecting twython\n",
      "  Downloading twython-3.8.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from twython) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.4.0 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from twython) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2019.11.28)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/eduardo/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
      "Installing collected packages: twython\n",
      "Successfully installed twython-3.8.2\n",
      "Requirement already satisfied: nltk in /home/eduardo/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/eduardo/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Pode ser necessário instalar esses pacotes\n",
    "!pip install requests_oauthlib\n",
    "!pip install twython\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote NLTK\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de update - a cada 5 Segundos a gente grava os resultados\n",
    "INTERVALO_BATCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o StreamingContext - Contexto de manipulacao do Streaming de dados\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o Classificador de Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle --> https://inclass.kaggle.com/c/si650winter11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse dataset contém 1,578,627 tweets classificados e cada linha é marcada como: \n",
    "\n",
    "### 1 para o sentimento positivo \n",
    "### 0 para o sentimento negativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo texto e criando um RDD em memória com Spark\n",
    "arquivo = sc.textFile(\"dataset_analise_sentimento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo o cabeçalho\n",
    "header = arquivo.take(1)[0]\n",
    "dataset = arquivo.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função separa as colunas de cada linha, cria uma tupla e remove a pontuação. Faz limpeza de dados\n",
    "def get_row(line):\n",
    "  row = line.split(',')\n",
    "  sentimento = row[1]\n",
    "  tweet = row[3].strip()\n",
    "  translator = str.maketrans({key: None for key in string.punctuation})\n",
    "  tweet = tweet.translate(translator)\n",
    "  tweet = tweet.split(' ')\n",
    "  tweet_lower = []\n",
    "  for word in tweet:\n",
    "    tweet_lower.append(word.lower())\n",
    "  return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a função a cada linha do dataset\n",
    "dataset_treino = dataset.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto SentimentAnalyzer \n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eduardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Certifique-se de ter espaço em disco - Aproximadamente 5GB\n",
    "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
    "nltk.download()\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ntlkdata.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'ntlkdata.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a lista de stopwords em Inglês \n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "  stopwords_all.append(word)\n",
    "  stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
    "dataset_treino_amostra = dataset_treino.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um unigram (n-grama) e extrai as features\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'contains()': False, 'contains(im)': False, 'contains(_NEG)': False, 'contains(followfriday)': False, 'contains(amp)': False, 'contains(dont)': False, 'contains(day)': False, 'contains(love)': False, 'contains(like)': False, 'contains(cant)': False, 'contains(good)': False, 'contains(get)': False, 'contains(go)': False, 'contains(today)': False, 'contains(got)': False, 'contains(want)': False, 'contains(time)': False, 'contains(going)': False, 'contains(back)': False, 'contains(one)': False, 'contains(sad)': True, 'contains(really)': False, 'contains(miss)': False, 'contains(u)': False, 'contains(work)': False, 'contains(new)': False, 'contains(2)': False, 'contains(last)': False, 'contains(still)': False, 'contains(twitter)': False, 'contains(night)': False, 'contains(great)': False, 'contains(lol)': False, 'contains(follow)': False, 'contains(need)': False, 'contains(see)': False, 'contains(much)': False, 'contains(myweakness)': False, 'contains(get_NEG)': False, 'contains(didnt)': False, 'contains(think)': False, 'contains(hate)': False, 'contains(iremember)': False, 'contains(home)': False, 'contains(feel)': False, 'contains(musicmonday)': False, 'contains(know)': False, 'contains(happy)': False, 'contains(people)': False, 'contains(lt3)': False, 'contains(would)': False, 'contains(bad)': False, 'contains(well)': False, 'contains(right)': False, 'contains(wish)': False, 'contains(oh)': False, 'contains(gonna)': False, 'contains(tomorrow)': False, 'contains(tonight)': False, 'contains(ff)': False, 'contains(ill)': False, 'contains(please)': False, 'contains(hope)': False, 'contains(thanks)': False, 'contains(morning)': False, 'contains(someone)': False, 'contains(never)': False, 'contains(ive)': False, 'contains(make)': False, 'contains(getting)': False, 'contains(im_NEG)': False, 'contains(go_NEG)': False, 'contains(know_NEG)': False, 'contains(awesome)': False, 'contains(like_NEG)': False, 'contains(inaperfectworld)': False, 'contains(thats)': False, 'contains(come)': False, 'contains(squarespace)': False, 'contains(wont)': False, 'contains(haha)': False, 'contains(lt)': False, 'contains(wanna)': False, 'contains(1)': False, 'contains(lost)': False, 'contains(days)': False, 'contains(4)': False, 'contains(makes)': False, 'contains(fun)': False, 'contains(friends)': False, 'contains(life)': False, 'contains(best)': False, 'contains(iphone)': False, 'contains(quoti)': False, 'contains(good_NEG)': False, 'contains(could)': False, 'contains(song)': False, 'contains(school)': False, 'contains(bed)': False, 'contains(better)': False, 'contains(dontyouhate)': False, 'contains(first)': False, 'contains(ever)': False, 'contains(3)': False, 'contains(us)': False, 'contains(haveyouever)': False, 'contains(sick)': False, 'contains(nice)': False, 'contains(man)': False, 'contains(want_NEG)': False, 'contains(doesnt)': False, 'contains(everyone)': False, 'contains(already)': False, 'contains(one_NEG)': False, 'contains(guys)': False, 'contains(show)': False, 'contains(sorry)': False, 'contains(week)': False, 'contains(music)': False, 'contains(things)': False, 'contains(old)': False, 'contains(bgt)': False, 'contains(next)': False, 'contains(made)': False, 'contains(something)': False, 'contains(n)': False, 'contains(way)': False, 'contains(another)': False, 'contains(gt)': False, 'contains(sleep)': False, 'contains(take)': False, 'contains(soon)': False, 'contains(baby)': False, 'contains(isnt)': False, 'contains(work_NEG)': False, 'contains(little)': False, 'contains(away)': False, 'contains(damn)': False, 'contains(quot)': False, 'contains(say)': False, 'contains(always)': False, 'contains(phone)': False, 'contains(left)': False, 'contains(shes)': False, 'contains(see_NEG)': False, 'contains(summer)': False, 'contains(weekend)': False, 'contains(year)': False, 'contains(today_NEG)': False, 'contains(amazing)': False, 'contains(wait_NEG)': False, 'contains(ok)': False, 'contains(long)': False, 'contains(girl)': False, 'contains(world)': False, 'contains(watching)': False, 'contains(movie)': False, 'contains(goodsex)': False, 'contains(feeling)': False, 'contains(ur)': False, 'contains(watch)': False, 'contains(cool)': False, 'contains(found)': False, 'contains(friend)': True, 'contains(mom)': False, 'contains(hes)': False, 'contains(friday)': False, 'contains(done)': False, 'contains(hours)': False, 'contains(said)': False, 'contains(went)': False, 'contains(gone)': False, 'contains(tired)': False, 'contains(house)': False, 'contains(missed)': False, 'contains(give)': False, 'contains(rain)': False, 'contains(leave)': False, 'contains(thing)': False, 'contains(wanted)': False, 'contains(head)': False, 'contains(sucks)': False, 'contains(sleep_NEG)': False, 'contains(ready)': False, 'contains(thank)': False, 'contains(guess)': False, 'contains(nothing)': False, 'contains(talk)': False, 'contains(followers)': False, 'contains(keep)': False, 'contains(tweets)': False, 'contains(look)': False, 'contains(hurts)': False, 'contains(early)': False, 'contains(game)': False, 'contains(two)': False, 'contains(guy)': False, 'contains(cry)': False, 'contains(going_NEG)': False, 'contains(live)': False}, '0'), ({'contains()': False, 'contains(im)': False, 'contains(_NEG)': False, 'contains(followfriday)': False, 'contains(amp)': False, 'contains(dont)': False, 'contains(day)': False, 'contains(love)': False, 'contains(like)': False, 'contains(cant)': False, 'contains(good)': False, 'contains(get)': False, 'contains(go)': False, 'contains(today)': False, 'contains(got)': False, 'contains(want)': False, 'contains(time)': False, 'contains(going)': False, 'contains(back)': False, 'contains(one)': False, 'contains(sad)': False, 'contains(really)': False, 'contains(miss)': False, 'contains(u)': False, 'contains(work)': False, 'contains(new)': True, 'contains(2)': False, 'contains(last)': False, 'contains(still)': False, 'contains(twitter)': False, 'contains(night)': False, 'contains(great)': False, 'contains(lol)': False, 'contains(follow)': False, 'contains(need)': False, 'contains(see)': False, 'contains(much)': False, 'contains(myweakness)': False, 'contains(get_NEG)': False, 'contains(didnt)': False, 'contains(think)': False, 'contains(hate)': False, 'contains(iremember)': False, 'contains(home)': False, 'contains(feel)': False, 'contains(musicmonday)': False, 'contains(know)': False, 'contains(happy)': False, 'contains(people)': False, 'contains(lt3)': False, 'contains(would)': False, 'contains(bad)': False, 'contains(well)': False, 'contains(right)': False, 'contains(wish)': False, 'contains(oh)': False, 'contains(gonna)': False, 'contains(tomorrow)': False, 'contains(tonight)': False, 'contains(ff)': False, 'contains(ill)': False, 'contains(please)': False, 'contains(hope)': False, 'contains(thanks)': False, 'contains(morning)': False, 'contains(someone)': False, 'contains(never)': False, 'contains(ive)': False, 'contains(make)': False, 'contains(getting)': False, 'contains(im_NEG)': False, 'contains(go_NEG)': False, 'contains(know_NEG)': False, 'contains(awesome)': False, 'contains(like_NEG)': False, 'contains(inaperfectworld)': False, 'contains(thats)': False, 'contains(come)': False, 'contains(squarespace)': False, 'contains(wont)': False, 'contains(haha)': False, 'contains(lt)': False, 'contains(wanna)': False, 'contains(1)': False, 'contains(lost)': False, 'contains(days)': False, 'contains(4)': False, 'contains(makes)': False, 'contains(fun)': False, 'contains(friends)': False, 'contains(life)': False, 'contains(best)': False, 'contains(iphone)': False, 'contains(quoti)': False, 'contains(good_NEG)': False, 'contains(could)': False, 'contains(song)': False, 'contains(school)': False, 'contains(bed)': False, 'contains(better)': False, 'contains(dontyouhate)': False, 'contains(first)': False, 'contains(ever)': False, 'contains(3)': False, 'contains(us)': False, 'contains(haveyouever)': False, 'contains(sick)': False, 'contains(nice)': False, 'contains(man)': False, 'contains(want_NEG)': False, 'contains(doesnt)': False, 'contains(everyone)': False, 'contains(already)': False, 'contains(one_NEG)': False, 'contains(guys)': False, 'contains(show)': False, 'contains(sorry)': False, 'contains(week)': False, 'contains(music)': False, 'contains(things)': False, 'contains(old)': False, 'contains(bgt)': False, 'contains(next)': False, 'contains(made)': False, 'contains(something)': False, 'contains(n)': False, 'contains(way)': False, 'contains(another)': False, 'contains(gt)': False, 'contains(sleep)': False, 'contains(take)': False, 'contains(soon)': False, 'contains(baby)': False, 'contains(isnt)': False, 'contains(work_NEG)': False, 'contains(little)': False, 'contains(away)': False, 'contains(damn)': False, 'contains(quot)': False, 'contains(say)': False, 'contains(always)': False, 'contains(phone)': False, 'contains(left)': False, 'contains(shes)': False, 'contains(see_NEG)': False, 'contains(summer)': False, 'contains(weekend)': False, 'contains(year)': False, 'contains(today_NEG)': False, 'contains(amazing)': False, 'contains(wait_NEG)': False, 'contains(ok)': False, 'contains(long)': False, 'contains(girl)': False, 'contains(world)': False, 'contains(watching)': False, 'contains(movie)': False, 'contains(goodsex)': False, 'contains(feeling)': False, 'contains(ur)': False, 'contains(watch)': False, 'contains(cool)': False, 'contains(found)': False, 'contains(friend)': False, 'contains(mom)': False, 'contains(hes)': False, 'contains(friday)': False, 'contains(done)': False, 'contains(hours)': False, 'contains(said)': False, 'contains(went)': False, 'contains(gone)': False, 'contains(tired)': False, 'contains(house)': False, 'contains(missed)': True, 'contains(give)': False, 'contains(rain)': False, 'contains(leave)': False, 'contains(thing)': False, 'contains(wanted)': False, 'contains(head)': False, 'contains(sucks)': False, 'contains(sleep_NEG)': False, 'contains(ready)': False, 'contains(thank)': False, 'contains(guess)': False, 'contains(nothing)': False, 'contains(talk)': False, 'contains(followers)': False, 'contains(keep)': False, 'contains(tweets)': False, 'contains(look)': False, 'contains(hurts)': False, 'contains(early)': False, 'contains(game)': False, 'contains(two)': False, 'contains(guy)': False, 'contains(cry)': False, 'contains(going_NEG)': False, 'contains(live)': False}, '0'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o classificador em algumas sentenças\n",
    "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticação do Twitter \n",
    "consumer_key = \"xxx\"\n",
    "consumer_secret = \"xxx\"\n",
    "access_token = \"xxx\"\n",
    "access_token_secret = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica a URL termo de busca\n",
    "search_term = 'Trump'\n",
    "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
    "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track='+search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto de atutenticação para o Twitter\n",
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o Stream\n",
    "rdd = ssc.sparkContext.parallelize([0])\n",
    "stream = ssc.queueStream([], default = rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de tweets por update\n",
    "NUM_TWEETS = 500  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "  response = requests.get(filter_url, auth = auth, stream = True)\n",
    "  print(filter_url, response)\n",
    "  count = 0\n",
    "  for line in response.iter_lines():\n",
    "    try:\n",
    "      if count > NUM_TWEETS:\n",
    "        break\n",
    "      post = json.loads(line.decode('utf-8'))\n",
    "      contents = [post['text']]\n",
    "      count += 1\n",
    "      yield str(contents)\n",
    "    except:\n",
    "      result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
    "def classifica_tweet(tweet):\n",
    "  sentence = [(tweet, '')]\n",
    "  test_set = sentiment_analyzer.apply_features(sentence)\n",
    "  print(tweet, classifier.classify(test_set[0][0]))\n",
    "  return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função retorna o texto do Twitter\n",
    "def get_tweet_text(rdd):\n",
    "  for line in rdd:\n",
    "    tweet = line.strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "      tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia para os resultados\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
    "def output_rdd(rdd):\n",
    "  global resultados\n",
    "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "  counts = pairs.reduceByKey(add)\n",
    "  output = []\n",
    "  for count in counts.collect():\n",
    "    output.append(count)\n",
    "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "  resultados.append(result)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03:11:23', []]\n",
      "['03:11:56', [('1', 96), ('0', 405)]]\n",
      "['03:12:17', [('0', 408), ('1', 93)]]\n",
      "['03:12:36', [('1', 90), ('0', 411)]]\n",
      "['03:12:54', [('1', 115), ('0', 386)]]\n",
      "['03:13:11', [('0', 390), ('1', 111)]]\n",
      "['03:13:27', [('0', 392), ('1', 109)]]\n",
      "['03:13:43', [('0', 411), ('1', 90)]]\n",
      "['03:13:57', [('1', 104), ('0', 397)]]\n",
      "['03:14:11', [('0', 403), ('1', 98)]]\n",
      "['03:14:25', [('1', 95), ('0', 406)]]\n",
      "['03:14:39', [('0', 415), ('1', 86)]]\n",
      "['03:14:52', [('0', 397), ('1', 104)]]\n",
      "['03:15:06', [('0', 408), ('1', 93)]]\n",
      "['03:15:19', [('0', 399), ('1', 102)]]\n",
      "['03:15:34', [('0', 390), ('1', 111)]]\n",
      "['03:15:49', [('0', 409), ('1', 92)]]\n",
      "['03:16:04', [('0', 397), ('1', 104)]]\n"
     ]
    }
   ],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "  if len(resultados) > 5:\n",
    "    cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03:16:21', [('0', 405), ('1', 96)]]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2672.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1096)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1067)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$1(PairRDDFunctions.scala:958)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 40, localhost, executor driver): java.io.IOException: Mkdirs failed to create file:/resultados/r031617/_temporary/0/_temporary/attempt_20200629151618_0171_m_000000_0 (exists=false, cwd=file:/home/eduardo/Downloads/spark-notebook)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:228)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:274)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 38 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/resultados/r031617/_temporary/0/_temporary/attempt_20200629151618_0171_m_000000_0 (exists=false, cwd=file:/home/eduardo/Downloads/spark-notebook)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:228)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-26006058942a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrdd_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/resultados/r'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%I%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresultados_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultados\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresultados_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2672.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1096)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1067)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$1(PairRDDFunctions.scala:958)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 40, localhost, executor driver): java.io.IOException: Mkdirs failed to create file:/resultados/r031617/_temporary/0/_temporary/attempt_20200629151618_0171_m_000000_0 (exists=false, cwd=file:/home/eduardo/Downloads/spark-notebook)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:228)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:274)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 38 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/resultados/r031617/_temporary/0/_temporary/attempt_20200629151618_0171_m_000000_0 (exists=false, cwd=file:/home/eduardo/Downloads/spark-notebook)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:228)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Grava os resultados\n",
    "rdd_save = '/resultados/r'+time.strftime(\"%I%M%S\")\n",
    "resultados_rdd = sc.parallelize(resultados)\n",
    "resultados_rdd.saveAsTextFile(rdd_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['03:11:23', []],\n",
       " ['03:11:56', [('1', 96), ('0', 405)]],\n",
       " ['03:12:17', [('0', 408), ('1', 93)]],\n",
       " ['03:12:36', [('1', 90), ('0', 411)]],\n",
       " ['03:12:54', [('1', 115), ('0', 386)]],\n",
       " ['03:13:11', [('0', 390), ('1', 111)]],\n",
       " ['03:13:27', [('0', 392), ('1', 109)]],\n",
       " ['03:13:43', [('0', 411), ('1', 90)]],\n",
       " ['03:13:57', [('1', 104), ('0', 397)]],\n",
       " ['03:14:11', [('0', 403), ('1', 98)]],\n",
       " ['03:14:25', [('1', 95), ('0', 406)]],\n",
       " ['03:14:39', [('0', 415), ('1', 86)]],\n",
       " ['03:14:52', [('0', 397), ('1', 104)]],\n",
       " ['03:15:06', [('0', 408), ('1', 93)]],\n",
       " ['03:15:19', [('0', 399), ('1', 102)]],\n",
       " ['03:15:34', [('0', 390), ('1', 111)]],\n",
       " ['03:15:49', [('0', 409), ('1', 92)]],\n",
       " ['03:16:04', [('0', 397), ('1', 104)]]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['03:16:43', [('1', 104), ('0', 397)]]\n",
      "['03:17:00', [('0', 396), ('1', 105)]]\n",
      "['03:17:14', [('0', 419), ('1', 82)]]\n"
     ]
    }
   ],
   "source": [
    "# Visualiza os resultados\n",
    "resultados_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaliza o streaming\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeu! - Ciencia dos Dados - <a href=\"http://facebook.com/cienciadosdadosbr\">facebook.com/cienciadosdadosbr</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram - Scripts e Datasets - Comunidade Telegram <a href=\"https://t.me/cienciadosdadosraiz\">https://t.me/cienciadosdadosraiz</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #**YouTube** - Mais Aulas como essa no YouTube <a href=\"https://www.youtube.com/watch?v=IaIc5oHd3II&t=1569s\">https://www.youtube.com/watch?v=IaIc5oHd3II&t=1569s</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"300\" height=\"200\" src=\"https://www.youtube.com/embed/okAOIlqSfUg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"300\" height=\"200\" src=\"https://www.youtube.com/embed/okAOIlqSfUg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
